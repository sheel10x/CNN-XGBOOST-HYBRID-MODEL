{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R67uqyLFzQRm",
        "outputId": "17209aa1-4137-4fa9-f08b-67aa241da895"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Data extracted successfully!\n",
            "Data path: /content/extracted_data/sartaj1\n",
            "Creating file paths list...\n",
            "Found 1621 images in glioma\n",
            "Found 1645 images in meningioma\n",
            "Found 2000 images in notumor\n",
            "Found 1457 images in pituitary\n",
            "Total files found: 6723\n",
            "Class distribution: [1621 1645 2000 1457]\n",
            "Creating enhanced CNN model with Swish activation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2-3249993399.py:136: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNet(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
            "\u001b[1m17225924/17225924\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Enhanced model created with Swish activation and 107 layers\n",
            "Trainable parameters: 9660356\n",
            "Train samples: 5378, Test samples: 1345\n",
            "Loading training images...\n",
            "Loaded 5378 training images in 10.64s\n",
            "Loading test images...\n",
            "Loaded 1345 test images in 3.40s\n",
            "Starting CNN training with Swish activation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
            "Expected: ['keras_tensor']\n",
            "Received: inputs=Tensor(shape=(None, 224, 224, 3))\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.5914 - loss: 2.0088\n",
            "Epoch 1: val_accuracy improved from -inf to 0.90632, saving model to best_brain_tumor_model_swish.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 250ms/step - accuracy: 0.5918 - loss: 2.0078 - val_accuracy: 0.9063 - val_loss: 1.1348 - learning_rate: 1.5000e-04\n",
            "Epoch 2/30\n",
            "\u001b[1m  1/336\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 46ms/step - accuracy: 0.9375 - loss: 1.2183"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2: val_accuracy did not improve from 0.90632\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9375 - loss: 1.2183 - val_accuracy: 0.9063 - val_loss: 1.1351 - learning_rate: 1.5000e-04\n",
            "Epoch 3/30\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.8615 - loss: 1.2707\n",
            "Epoch 3: val_accuracy improved from 0.90632 to 0.91970, saving model to best_brain_tumor_model_swish.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 197ms/step - accuracy: 0.8615 - loss: 1.2707 - val_accuracy: 0.9197 - val_loss: 1.1360 - learning_rate: 1.5000e-04\n",
            "Epoch 4/30\n",
            "\u001b[1m  1/336\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 36ms/step - accuracy: 0.9375 - loss: 1.0131\n",
            "Epoch 4: val_accuracy improved from 0.91970 to 0.92416, saving model to best_brain_tumor_model_swish.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9375 - loss: 1.0131 - val_accuracy: 0.9242 - val_loss: 1.1301 - learning_rate: 1.5000e-04\n",
            "Epoch 5/30\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.8783 - loss: 1.2212\n",
            "Epoch 5: val_accuracy improved from 0.92416 to 0.94796, saving model to best_brain_tumor_model_swish.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 197ms/step - accuracy: 0.8783 - loss: 1.2211 - val_accuracy: 0.9480 - val_loss: 1.0321 - learning_rate: 1.5000e-04\n",
            "Epoch 6/30\n",
            "\u001b[1m  1/336\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 41ms/step - accuracy: 0.9375 - loss: 1.0887\n",
            "Epoch 6: val_accuracy did not improve from 0.94796\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9375 - loss: 1.0887 - val_accuracy: 0.9472 - val_loss: 1.0315 - learning_rate: 1.5000e-04\n",
            "Epoch 7/30\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.9184 - loss: 1.1386\n",
            "Epoch 7: val_accuracy improved from 0.94796 to 0.95167, saving model to best_brain_tumor_model_swish.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 194ms/step - accuracy: 0.9184 - loss: 1.1386 - val_accuracy: 0.9517 - val_loss: 1.0197 - learning_rate: 1.5000e-04\n",
            "Epoch 8/30\n",
            "\u001b[1m  1/336\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 35ms/step - accuracy: 0.8750 - loss: 1.2660\n",
            "Epoch 8: val_accuracy did not improve from 0.95167\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8750 - loss: 1.2660 - val_accuracy: 0.9487 - val_loss: 1.0258 - learning_rate: 1.5000e-04\n",
            "Epoch 9/30\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.9235 - loss: 1.1125\n",
            "Epoch 9: val_accuracy improved from 0.95167 to 0.96877, saving model to best_brain_tumor_model_swish.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 191ms/step - accuracy: 0.9235 - loss: 1.1125 - val_accuracy: 0.9688 - val_loss: 0.9707 - learning_rate: 1.5000e-04\n",
            "Epoch 10/30\n",
            "\u001b[1m  1/336\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 0.9680\n",
            "Epoch 10: val_accuracy did not improve from 0.96877\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.9680 - val_accuracy: 0.9688 - val_loss: 0.9713 - learning_rate: 1.5000e-04\n",
            "Epoch 11/30\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.9403 - loss: 1.0567\n",
            "Epoch 11: val_accuracy did not improve from 0.96877\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 193ms/step - accuracy: 0.9403 - loss: 1.0566 - val_accuracy: 0.9643 - val_loss: 0.9597 - learning_rate: 1.5000e-04\n",
            "Epoch 12/30\n",
            "\u001b[1m  1/336\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - accuracy: 0.9375 - loss: 0.9701\n",
            "Epoch 12: val_accuracy did not improve from 0.96877\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9375 - loss: 0.9701 - val_accuracy: 0.9643 - val_loss: 0.9596 - learning_rate: 1.5000e-04\n",
            "Epoch 13/30\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.9516 - loss: 0.9999\n",
            "Epoch 13: val_accuracy improved from 0.96877 to 0.97918, saving model to best_brain_tumor_model_swish.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 194ms/step - accuracy: 0.9516 - loss: 1.0000 - val_accuracy: 0.9792 - val_loss: 0.9336 - learning_rate: 1.5000e-04\n",
            "Epoch 14/30\n",
            "\u001b[1m  1/336\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 57ms/step - accuracy: 0.9375 - loss: 1.0418\n",
            "Epoch 14: val_accuracy did not improve from 0.97918\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9375 - loss: 1.0418 - val_accuracy: 0.9792 - val_loss: 0.9331 - learning_rate: 1.5000e-04\n",
            "Epoch 15/30\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.9591 - loss: 0.9740\n",
            "Epoch 15: val_accuracy did not improve from 0.97918\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 191ms/step - accuracy: 0.9591 - loss: 0.9740 - val_accuracy: 0.9717 - val_loss: 0.9056 - learning_rate: 1.5000e-04\n",
            "Epoch 16/30\n",
            "\u001b[1m  1/336\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.8555\n",
            "Epoch 16: val_accuracy did not improve from 0.97918\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.8555 - val_accuracy: 0.9710 - val_loss: 0.9050 - learning_rate: 1.5000e-04\n",
            "Epoch 17/30\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.9693 - loss: 0.9271\n",
            "Epoch 17: val_accuracy did not improve from 0.97918\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 190ms/step - accuracy: 0.9693 - loss: 0.9271 - val_accuracy: 0.9784 - val_loss: 0.8708 - learning_rate: 1.5000e-04\n",
            "Epoch 18/30\n",
            "\u001b[1m  1/336\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - accuracy: 0.6875 - loss: 1.6452\n",
            "Epoch 18: val_accuracy did not improve from 0.97918\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6875 - loss: 1.6452 - val_accuracy: 0.9784 - val_loss: 0.8709 - learning_rate: 1.5000e-04\n",
            "Epoch 19/30\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - accuracy: 0.9707 - loss: 0.9121\n",
            "Epoch 19: val_accuracy improved from 0.97918 to 0.98141, saving model to best_brain_tumor_model_swish.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 197ms/step - accuracy: 0.9707 - loss: 0.9121 - val_accuracy: 0.9814 - val_loss: 0.8460 - learning_rate: 1.5000e-04\n",
            "Epoch 20/30\n",
            "\u001b[1m  1/336\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.8121\n",
            "Epoch 20: val_accuracy did not improve from 0.98141\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.8121 - val_accuracy: 0.9814 - val_loss: 0.8439 - learning_rate: 1.5000e-04\n",
            "Epoch 21/30\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.9692 - loss: 0.8856\n",
            "Epoch 21: val_accuracy did not improve from 0.98141\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 191ms/step - accuracy: 0.9692 - loss: 0.8857 - val_accuracy: 0.9807 - val_loss: 0.8482 - learning_rate: 1.5000e-04\n",
            "Epoch 22/30\n",
            "\u001b[1m  1/336\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.8133\n",
            "Epoch 22: val_accuracy did not improve from 0.98141\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.8133 - val_accuracy: 0.9807 - val_loss: 0.8474 - learning_rate: 1.5000e-04\n",
            "Epoch 23/30\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.9629 - loss: 0.8952\n",
            "Epoch 23: val_accuracy did not improve from 0.98141\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 195ms/step - accuracy: 0.9629 - loss: 0.8953 - val_accuracy: 0.9814 - val_loss: 0.8284 - learning_rate: 1.5000e-04\n",
            "Epoch 24/30\n",
            "\u001b[1m  1/336\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - accuracy: 0.9375 - loss: 1.0118\n",
            "Epoch 24: val_accuracy did not improve from 0.98141\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9375 - loss: 1.0118 - val_accuracy: 0.9814 - val_loss: 0.8286 - learning_rate: 1.5000e-04\n",
            "Epoch 25/30\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.9707 - loss: 0.8515\n",
            "Epoch 25: val_accuracy did not improve from 0.98141\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 195ms/step - accuracy: 0.9707 - loss: 0.8515 - val_accuracy: 0.9695 - val_loss: 0.8329 - learning_rate: 1.5000e-04\n",
            "Epoch 26/30\n",
            "\u001b[1m  1/336\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 35ms/step - accuracy: 0.9375 - loss: 0.8385\n",
            "Epoch 26: val_accuracy did not improve from 0.98141\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9375 - loss: 0.8385 - val_accuracy: 0.9703 - val_loss: 0.8285 - learning_rate: 1.5000e-04\n",
            "Epoch 27/30\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.9642 - loss: 0.8626\n",
            "Epoch 27: val_accuracy improved from 0.98141 to 0.99182, saving model to best_brain_tumor_model_swish.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 194ms/step - accuracy: 0.9642 - loss: 0.8625 - val_accuracy: 0.9918 - val_loss: 0.7576 - learning_rate: 1.5000e-04\n",
            "Epoch 28/30\n",
            "\u001b[1m  1/336\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.7609\n",
            "Epoch 28: val_accuracy did not improve from 0.99182\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.7609 - val_accuracy: 0.9911 - val_loss: 0.7581 - learning_rate: 1.5000e-04\n",
            "Epoch 29/30\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.9594 - loss: 0.8674\n",
            "Epoch 29: val_accuracy did not improve from 0.99182\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 196ms/step - accuracy: 0.9594 - loss: 0.8675 - val_accuracy: 0.9792 - val_loss: 0.7872 - learning_rate: 1.5000e-04\n",
            "Epoch 30/30\n",
            "\u001b[1m  1/336\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.7993\n",
            "Epoch 30: val_accuracy did not improve from 0.99182\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.7993 - val_accuracy: 0.9792 - val_loss: 0.7871 - learning_rate: 1.5000e-04\n",
            "Restoring model weights from the end of the best epoch: 27.\n",
            "CNN training completed in 1187.11s\n",
            "Loading best CNN model weights...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating feature extractor from best CNN weights...\n",
            "Feature extractor created - extracting 128 features\n",
            "Extracting features for XGBoost training...\n",
            "Extracting training features in batches...\n",
            "Processed batch 10/169\n",
            "Processed batch 20/169\n",
            "Processed batch 30/169\n",
            "Processed batch 40/169\n",
            "Processed batch 50/169\n",
            "Processed batch 60/169\n",
            "Processed batch 70/169\n",
            "Processed batch 80/169\n",
            "Processed batch 90/169\n",
            "Processed batch 100/169\n",
            "Processed batch 110/169\n",
            "Processed batch 120/169\n",
            "Processed batch 130/169\n",
            "Processed batch 140/169\n",
            "Processed batch 150/169\n",
            "Processed batch 160/169\n",
            "Processed batch 169/169\n",
            "Extracting test features in batches...\n",
            "Processed batch 10/43\n",
            "Processed batch 20/43\n",
            "Processed batch 30/43\n",
            "Processed batch 40/43\n",
            "Processed batch 43/43\n",
            "Feature extraction completed in 90.78s\n",
            "Feature shape: (5378, 128)\n",
            "Standardizing features for XGBoost...\n",
            "Training XGBoost with parameter tuning...\n",
            "Testing 8 XGBoost parameter combinations...\n",
            "================================================================================\n",
            "\n",
            "Testing Combination 1/8:\n",
            "Parameters: {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.8, 'reg_alpha': 0.1, 'reg_lambda': 1.0}\n",
            "Accuracy: 0.9918\n",
            "Precision: 0.9920\n",
            "Recall: 0.9918\n",
            "F1-Score: 0.9918\n",
            "MCC: 0.9891\n",
            "Training Time: 2.00s\n",
            "Prediction Time: 0.01s\n",
            "*** NEW BEST ACCURACY: 0.9918 ***\n",
            "------------------------------------------------------------\n",
            "\n",
            "Testing Combination 2/8:\n",
            "Parameters: {'n_estimators': 150, 'max_depth': 4, 'learning_rate': 0.05, 'subsample': 0.7, 'colsample_bytree': 0.7, 'reg_alpha': 0.5, 'reg_lambda': 2.0}\n",
            "Accuracy: 0.9926\n",
            "Precision: 0.9927\n",
            "Recall: 0.9926\n",
            "F1-Score: 0.9926\n",
            "MCC: 0.9901\n",
            "Training Time: 2.85s\n",
            "Prediction Time: 0.02s\n",
            "*** NEW BEST ACCURACY: 0.9926 ***\n",
            "------------------------------------------------------------\n",
            "\n",
            "Testing Combination 3/8:\n",
            "Parameters: {'n_estimators': 200, 'max_depth': 8, 'learning_rate': 0.15, 'subsample': 0.9, 'colsample_bytree': 0.9, 'reg_alpha': 0.01, 'reg_lambda': 0.5}\n",
            "Accuracy: 0.9926\n",
            "Precision: 0.9926\n",
            "Recall: 0.9926\n",
            "F1-Score: 0.9926\n",
            "MCC: 0.9900\n",
            "Training Time: 6.21s\n",
            "Prediction Time: 0.01s\n",
            "------------------------------------------------------------\n",
            "\n",
            "Testing Combination 4/8:\n",
            "Parameters: {'n_estimators': 80, 'max_depth': 3, 'learning_rate': 0.2, 'subsample': 0.8, 'colsample_bytree': 0.8, 'reg_alpha': 0.3, 'reg_lambda': 1.5}\n",
            "Accuracy: 0.9926\n",
            "Precision: 0.9926\n",
            "Recall: 0.9926\n",
            "F1-Score: 0.9926\n",
            "MCC: 0.9901\n",
            "Training Time: 1.17s\n",
            "Prediction Time: 0.01s\n",
            "------------------------------------------------------------\n",
            "\n",
            "Testing Combination 5/8:\n",
            "Parameters: {'n_estimators': 300, 'max_depth': 10, 'learning_rate': 0.03, 'subsample': 0.8, 'colsample_bytree': 0.8, 'reg_alpha': 0.2, 'reg_lambda': 1.0}\n",
            "Accuracy: 0.9918\n",
            "Precision: 0.9920\n",
            "Recall: 0.9918\n",
            "F1-Score: 0.9918\n",
            "MCC: 0.9891\n",
            "Training Time: 5.80s\n",
            "Prediction Time: 0.03s\n",
            "------------------------------------------------------------\n",
            "\n",
            "Testing Combination 6/8:\n",
            "Parameters: {'n_estimators': 120, 'max_depth': 5, 'learning_rate': 0.08, 'subsample': 0.75, 'colsample_bytree': 0.75, 'reg_alpha': 1.0, 'reg_lambda': 3.0}\n",
            "Accuracy: 0.9926\n",
            "Precision: 0.9927\n",
            "Recall: 0.9926\n",
            "F1-Score: 0.9926\n",
            "MCC: 0.9901\n",
            "Training Time: 4.46s\n",
            "Prediction Time: 0.01s\n",
            "------------------------------------------------------------\n",
            "\n",
            "Testing Combination 7/8:\n",
            "Parameters: {'n_estimators': 50, 'max_depth': 4, 'learning_rate': 0.3, 'subsample': 0.9, 'colsample_bytree': 0.9, 'reg_alpha': 0.1, 'reg_lambda': 0.5}\n",
            "Accuracy: 0.9926\n",
            "Precision: 0.9926\n",
            "Recall: 0.9926\n",
            "F1-Score: 0.9926\n",
            "MCC: 0.9901\n",
            "Training Time: 0.83s\n",
            "Prediction Time: 0.01s\n",
            "------------------------------------------------------------\n",
            "\n",
            "Testing Combination 8/8:\n",
            "Parameters: {'n_estimators': 250, 'max_depth': 7, 'learning_rate': 0.06, 'subsample': 0.85, 'colsample_bytree': 0.85, 'reg_alpha': 0.3, 'reg_lambda': 1.2}\n",
            "Accuracy: 0.9903\n",
            "Precision: 0.9904\n",
            "Recall: 0.9903\n",
            "F1-Score: 0.9903\n",
            "MCC: 0.9871\n",
            "Training Time: 4.06s\n",
            "Prediction Time: 0.02s\n",
            "------------------------------------------------------------\n",
            "\n",
            "Best XGBoost Configuration Found:\n",
            "Best Accuracy: 0.9926\n",
            "Best Parameters: {'n_estimators': 150, 'max_depth': 4, 'learning_rate': 0.05, 'subsample': 0.7, 'colsample_bytree': 0.7, 'reg_alpha': 0.5, 'reg_lambda': 2.0}\n",
            "\n",
            "================================================================================\n",
            "CNN TRAINING RESULTS (WITH SWISH ACTIVATION)\n",
            "================================================================================\n",
            "Highest CNN Training Accuracy: 1.0000\n",
            "Highest CNN Validation Accuracy: 0.9918\n",
            "CNN Training Time: 1187.11s\n",
            "\n",
            "================================================================================\n",
            "CNN + XGBOOST HYBRID CLASSIFICATION RESULTS\n",
            "================================================================================\n",
            "XGBoost Accuracy: 0.9926\n",
            "XGBoost Precision: 0.9927\n",
            "XGBoost Recall: 0.9926\n",
            "XGBoost F1-Score: 0.9926\n",
            "Matthews Correlation Coefficient: 0.9901\n",
            "\n",
            "Dataset Information:\n",
            "Training samples: 5378\n",
            "Test samples: 1345\n",
            "Features extracted: 128\n",
            "\n",
            "Timing Information:\n",
            "Feature extraction time: 90.78s\n",
            "XGBoost training time: 2.85s\n",
            "XGBoost prediction time: 0.02s\n",
            "Total training time: 1189.96s\n",
            "\n",
            "Best XGBoost Parameters:\n",
            "  n_estimators: 150\n",
            "  max_depth: 4\n",
            "  learning_rate: 0.05\n",
            "  subsample: 0.7\n",
            "  colsample_bytree: 0.7\n",
            "  reg_alpha: 0.5\n",
            "  reg_lambda: 2.0\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Glioma     0.9969    0.9846    0.9907       324\n",
            "  Meningioma     0.9762    0.9970    0.9865       329\n",
            "    No Tumor     1.0000    0.9950    0.9975       400\n",
            "   Pituitary     0.9966    0.9932    0.9949       292\n",
            "\n",
            "    accuracy                         0.9926      1345\n",
            "   macro avg     0.9924    0.9924    0.9924      1345\n",
            "weighted avg     0.9927    0.9926    0.9926      1345\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "Predicted ->\n",
            "Actual       Glioma   Meningioma   No Tumor   Pituitary \n",
            "------------------------------------------------------------\n",
            "Glioma       319      5            0          0         \n",
            "Meningioma   1        328          0          0         \n",
            "No Tumor     0        1            398        1         \n",
            "Pituitary    0        2            0          290       \n",
            "\n",
            "================================================================================\n",
            "FINAL SUMMARY - CNN + XGBOOST HYBRID MODEL\n",
            "================================================================================\n",
            "Feature Extractor: Enhanced MobileNet CNN with Swish activation\n",
            "Classifier: XGBoost (optimized parameters)\n",
            "CNN Highest Training Accuracy: 1.0000\n",
            "CNN Highest Validation Accuracy: 0.9918\n",
            "XGBoost Final Test Accuracy: 0.9926\n",
            "XGBoost Final Precision: 0.9927\n",
            "XGBoost Final Recall: 0.9926\n",
            "XGBoost Final F1-Score: 0.9926\n",
            "Matthews Correlation Coefficient: 0.9901\n",
            "Total Training Time: 1189.96s\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Input\n",
        "from keras.applications import MobileNet\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_recall_fscore_support, matthews_corrcoef\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from zipfile import ZipFile\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dropout, Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import gc  # Added for garbage collection\n",
        "import xgboost as xgb\n",
        "from itertools import product\n",
        "\n",
        "# Set random seeds function\n",
        "def set_seeds(seed_value):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    tf.random.set_seed(seed_value)\n",
        "\n",
        "seed_value = 42\n",
        "set_seeds(seed_value)\n",
        "\n",
        "# Mount Google Drive and extract data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Extract zip file\n",
        "zip_path = '/content/drive/My Drive/sartaj1.zip'\n",
        "extract_path = '/content/extracted_data'\n",
        "\n",
        "try:\n",
        "    with ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"Data extracted successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error extracting data: {e}\")\n",
        "\n",
        "# Find the correct data path\n",
        "def find_data_path():\n",
        "    categories = [\"glioma\", \"meningioma\", \"notumor\", \"pituitary\"]\n",
        "    for root, dirs, files in os.walk(extract_path):\n",
        "        if all(category in dirs for category in categories):\n",
        "            return root\n",
        "    return extract_path\n",
        "\n",
        "data_path = find_data_path()\n",
        "print(f\"Data path: {data_path}\")\n",
        "\n",
        "categories = [\"glioma\", \"meningioma\", \"notumor\", \"pituitary\"]\n",
        "labels = [\"Glioma\", \"Meningioma\", \"No Tumor\", \"Pituitary\"]\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Create file paths list\n",
        "def create_file_paths_list():\n",
        "    \"\"\"Create list of file paths and labels WITHOUT loading images\"\"\"\n",
        "    file_paths = []\n",
        "    file_labels = []\n",
        "\n",
        "    for category in categories:\n",
        "        path = os.path.join(data_path, category)\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"Warning: Path {path} does not exist!\")\n",
        "            continue\n",
        "\n",
        "        class_num = categories.index(category)\n",
        "        image_files = [f for f in os.listdir(path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        print(f\"Found {len(image_files)} images in {category}\")\n",
        "\n",
        "        for img_file in image_files:\n",
        "            file_paths.append(os.path.join(path, img_file))\n",
        "            file_labels.append(class_num)\n",
        "\n",
        "    print(f\"Total files found: {len(file_paths)}\")\n",
        "    print(f\"Class distribution: {np.bincount(file_labels)}\")\n",
        "\n",
        "    # Shuffle the data\n",
        "    combined = list(zip(file_paths, file_labels))\n",
        "    random.shuffle(combined)\n",
        "    file_paths, file_labels = zip(*combined)\n",
        "\n",
        "    return np.array(file_paths), np.array(file_labels)\n",
        "\n",
        "# Enhanced image loading with better preprocessing\n",
        "def load_image_batch(file_paths, labels, indices, img_size=224):\n",
        "    \"\"\"Load only a batch of images from file paths with enhanced preprocessing\"\"\"\n",
        "    batch_images = []\n",
        "    batch_labels = []\n",
        "    valid_indices = []\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        try:\n",
        "            img_path = file_paths[idx]\n",
        "            img_array = cv2.imread(img_path)\n",
        "            if img_array is not None:\n",
        "                # Convert BGR to RGB\n",
        "                img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)\n",
        "                img_array = cv2.resize(img_array, (img_size, img_size))\n",
        "\n",
        "                # Enhanced preprocessing - histogram equalization for better contrast\n",
        "                img_array = cv2.convertScaleAbs(img_array, alpha=1.1, beta=10)\n",
        "\n",
        "                img_array = img_array.astype(np.float32) / 255.0  # Normalize\n",
        "                batch_images.append(img_array)\n",
        "                batch_labels.append(labels[idx])\n",
        "                valid_indices.append(idx)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {idx}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return np.array(batch_images), np.array(batch_labels), valid_indices\n",
        "\n",
        "# Enhanced CNN model with Swish activation function\n",
        "def create_enhanced_model():\n",
        "    \"\"\"\n",
        "    Create enhanced CNN model with MobileNet base + custom layers\n",
        "    Using Swish activation function instead of ReLU\n",
        "    \"\"\"\n",
        "    from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dropout, Dense, Flatten\n",
        "    from tensorflow.keras.regularizers import l2\n",
        "\n",
        "    input_tensor = Input(shape=(224, 224, 3))\n",
        "\n",
        "    # Base Model - MobileNet\n",
        "    base_model = MobileNet(\n",
        "        input_tensor=input_tensor,\n",
        "        weights='imagenet',\n",
        "        include_top=False  # Remove classification layers\n",
        "    )\n",
        "\n",
        "    # More aggressive fine-tuning - unfreeze more layers\n",
        "    for layer in base_model.layers[:-30]:  # Freeze fewer layers (changed from -20 to -30)\n",
        "        layer.trainable = False\n",
        "    for layer in base_model.layers[-30:]:  # Unfreeze last 30 layers (changed from 20 to 30)\n",
        "        layer.trainable = True\n",
        "\n",
        "    # Add custom layers with Swish activation\n",
        "    x = base_model.output\n",
        "\n",
        "    # Custom Conv2D(512) + BatchNorm + MaxPool + Dropout with Swish\n",
        "    x = Conv2D(512, (3, 3), activation='swish', padding='same', kernel_regularizer=l2(0.0005))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # Custom Conv2D(256) + BatchNorm + MaxPool + Dropout with Swish\n",
        "    x = Conv2D(256, (3, 3), activation='swish', padding='same', kernel_regularizer=l2(0.0005))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # Custom Conv2D(128) + BatchNorm + Dropout with Swish\n",
        "    x = Conv2D(128, (3, 3), activation='swish', padding='same', kernel_regularizer=l2(0.0005))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # GlobalAveragePooling2D\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "    # Dense layers with Swish activation\n",
        "    x = Dense(512, activation='swish', kernel_regularizer=l2(0.0005))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Dense(256, activation='swish', kernel_regularizer=l2(0.0005))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Dense(128, activation='swish', kernel_regularizer=l2(0.0005))(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    # Final classification layer\n",
        "    predictions = Dense(4, activation='softmax')(x)  # 4 classes for brain tumor types\n",
        "\n",
        "    # Create the complete model\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    print(f\"Enhanced model created with Swish activation and {len(model.layers)} layers\")\n",
        "    print(f\"Trainable parameters: {model.count_params()}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create feature extractor from trained CNN\n",
        "def create_feature_extractor(trained_model):\n",
        "    \"\"\"\n",
        "    Create a feature extractor from the trained CNN model\n",
        "    Extract features from the last dense layer before classification\n",
        "    \"\"\"\n",
        "    # Get the layer before the final classification layer\n",
        "    feature_layer = trained_model.layers[-2]  # Second to last layer (before softmax)\n",
        "\n",
        "    # Create feature extractor model\n",
        "    feature_extractor = Model(\n",
        "        inputs=trained_model.input,\n",
        "        outputs=feature_layer.output\n",
        "    )\n",
        "\n",
        "    print(f\"Feature extractor created - extracting {feature_layer.output.shape[-1]} features\")\n",
        "    return feature_extractor\n",
        "\n",
        "# Batch-wise feature extraction function\n",
        "def extract_features_in_batches(feature_extractor, X_data, batch_size=32):\n",
        "    \"\"\"\n",
        "    Extract features in batches to avoid memory issues\n",
        "    \"\"\"\n",
        "    n_samples = len(X_data)\n",
        "    n_batches = (n_samples + batch_size - 1) // batch_size\n",
        "\n",
        "    features_list = []\n",
        "\n",
        "    for i in range(n_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = min((i + 1) * batch_size, n_samples)\n",
        "\n",
        "        batch_data = X_data[start_idx:end_idx]\n",
        "\n",
        "        # Extract features for this batch\n",
        "        batch_features = feature_extractor.predict(batch_data, verbose=0)\n",
        "        features_list.append(batch_features)\n",
        "\n",
        "        # Force garbage collection to free memory\n",
        "        gc.collect()\n",
        "\n",
        "        # Print progress\n",
        "        if (i + 1) % 10 == 0 or i == n_batches - 1:\n",
        "            print(f\"Processed batch {i+1}/{n_batches}\")\n",
        "\n",
        "    # Concatenate all features\n",
        "    all_features = np.concatenate(features_list, axis=0)\n",
        "\n",
        "    return all_features\n",
        "\n",
        "# XGBoost parameter combinations for grid search\n",
        "def get_xgboost_param_combinations():\n",
        "    \"\"\"\n",
        "    Define different XGBoost parameter combinations to test\n",
        "    \"\"\"\n",
        "    param_combinations = [\n",
        "        # Combination 1: Balanced performance\n",
        "        {\n",
        "            'n_estimators': 100,\n",
        "            'max_depth': 6,\n",
        "            'learning_rate': 0.1,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.8,\n",
        "            'reg_alpha': 0.1,\n",
        "            'reg_lambda': 1.0\n",
        "        },\n",
        "        # Combination 2: More conservative (prevent overfitting)\n",
        "        {\n",
        "            'n_estimators': 150,\n",
        "            'max_depth': 4,\n",
        "            'learning_rate': 0.05,\n",
        "            'subsample': 0.7,\n",
        "            'colsample_bytree': 0.7,\n",
        "            'reg_alpha': 0.5,\n",
        "            'reg_lambda': 2.0\n",
        "        },\n",
        "        # Combination 3: More aggressive (higher capacity)\n",
        "        {\n",
        "            'n_estimators': 200,\n",
        "            'max_depth': 8,\n",
        "            'learning_rate': 0.15,\n",
        "            'subsample': 0.9,\n",
        "            'colsample_bytree': 0.9,\n",
        "            'reg_alpha': 0.01,\n",
        "            'reg_lambda': 0.5\n",
        "        },\n",
        "        # Combination 4: High learning rate, shallow trees\n",
        "        {\n",
        "            'n_estimators': 80,\n",
        "            'max_depth': 3,\n",
        "            'learning_rate': 0.2,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.8,\n",
        "            'reg_alpha': 0.3,\n",
        "            'reg_lambda': 1.5\n",
        "        },\n",
        "        # Combination 5: Low learning rate, deep trees\n",
        "        {\n",
        "            'n_estimators': 300,\n",
        "            'max_depth': 10,\n",
        "            'learning_rate': 0.03,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.8,\n",
        "            'reg_alpha': 0.2,\n",
        "            'reg_lambda': 1.0\n",
        "        },\n",
        "        # Combination 6: Moderate settings with high regularization\n",
        "        {\n",
        "            'n_estimators': 120,\n",
        "            'max_depth': 5,\n",
        "            'learning_rate': 0.08,\n",
        "            'subsample': 0.75,\n",
        "            'colsample_bytree': 0.75,\n",
        "            'reg_alpha': 1.0,\n",
        "            'reg_lambda': 3.0\n",
        "        },\n",
        "        # Combination 7: Fast training setup\n",
        "        {\n",
        "            'n_estimators': 50,\n",
        "            'max_depth': 4,\n",
        "            'learning_rate': 0.3,\n",
        "            'subsample': 0.9,\n",
        "            'colsample_bytree': 0.9,\n",
        "            'reg_alpha': 0.1,\n",
        "            'reg_lambda': 0.5\n",
        "        },\n",
        "        # Combination 8: Comprehensive model\n",
        "        {\n",
        "            'n_estimators': 250,\n",
        "            'max_depth': 7,\n",
        "            'learning_rate': 0.06,\n",
        "            'subsample': 0.85,\n",
        "            'colsample_bytree': 0.85,\n",
        "            'reg_alpha': 0.3,\n",
        "            'reg_lambda': 1.2\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    return param_combinations\n",
        "\n",
        "# Train XGBoost with parameter tuning\n",
        "def train_xgboost_with_tuning(X_train_features, y_train, X_test_features, y_test):\n",
        "    \"\"\"\n",
        "    Train XGBoost with different parameter combinations and find the best one\n",
        "    \"\"\"\n",
        "    param_combinations = get_xgboost_param_combinations()\n",
        "    best_accuracy = 0\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "    best_results = None\n",
        "\n",
        "    print(f\"Testing {len(param_combinations)} XGBoost parameter combinations...\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for i, params in enumerate(param_combinations, 1):\n",
        "        print(f\"\\nTesting Combination {i}/{len(param_combinations)}:\")\n",
        "        print(f\"Parameters: {params}\")\n",
        "\n",
        "        # Train XGBoost with current parameters\n",
        "        start_time = time.time()\n",
        "        xgb_model = xgb.XGBClassifier(\n",
        "            objective='multi:softprob',\n",
        "            num_class=4,\n",
        "            random_state=seed_value,\n",
        "            n_jobs=-1,\n",
        "            **params\n",
        "        )\n",
        "\n",
        "        xgb_model.fit(X_train_features, y_train)\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        # Make predictions\n",
        "        start_time = time.time()\n",
        "        y_pred = xgb_model.predict(X_test_features)\n",
        "        y_pred_proba = xgb_model.predict_proba(X_test_features)\n",
        "        prediction_time = time.time() - start_time\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
        "        mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "        current_results = {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'mcc': mcc,\n",
        "            'training_time': training_time,\n",
        "            'prediction_time': prediction_time,\n",
        "            'params': params,\n",
        "            'model': xgb_model,\n",
        "            'y_pred': y_pred,\n",
        "            'y_pred_proba': y_pred_proba\n",
        "        }\n",
        "\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1-Score: {f1:.4f}\")\n",
        "        print(f\"MCC: {mcc:.4f}\")\n",
        "        print(f\"Training Time: {training_time:.2f}s\")\n",
        "        print(f\"Prediction Time: {prediction_time:.2f}s\")\n",
        "\n",
        "        # Update best results if current is better\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_params = params\n",
        "            best_model = xgb_model\n",
        "            best_results = current_results\n",
        "            print(f\"*** NEW BEST ACCURACY: {accuracy:.4f} ***\")\n",
        "\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "    print(f\"\\nBest XGBoost Configuration Found:\")\n",
        "    print(f\"Best Accuracy: {best_accuracy:.4f}\")\n",
        "    print(f\"Best Parameters: {best_params}\")\n",
        "\n",
        "    return best_model, best_results, best_params\n",
        "\n",
        "# Enhanced training with CNN + XGBoost hybrid approach\n",
        "def train_cnn_xgboost_hybrid(file_paths, file_labels, test_size=0.2, batch_size=16, epochs=30):\n",
        "    \"\"\"\n",
        "    Train CNN model first, then use best weights as feature extractor for XGBoost\n",
        "    \"\"\"\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "    from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "    # Create enhanced model\n",
        "    print(\"Creating enhanced CNN model with Swish activation...\")\n",
        "    cnn_model = create_enhanced_model()\n",
        "\n",
        "    # Fine-tuned optimizer with slightly higher learning rate\n",
        "    cnn_model.compile(\n",
        "        optimizer=Adam(learning_rate=0.00015, beta_1=0.9, beta_2=0.999),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Split data into train and test\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        range(len(file_paths)),\n",
        "        test_size=test_size,\n",
        "        random_state=seed_value,\n",
        "        stratify=file_labels\n",
        "    )\n",
        "\n",
        "    print(f\"Train samples: {len(train_idx)}, Test samples: {len(test_idx)}\")\n",
        "\n",
        "    # Load training images\n",
        "    print(\"Loading training images...\")\n",
        "    start_time = time.time()\n",
        "    X_train, y_train, valid_train_idx = load_image_batch(\n",
        "        file_paths, file_labels, train_idx, IMG_SIZE\n",
        "    )\n",
        "    print(f\"Loaded {len(X_train)} training images in {time.time() - start_time:.2f}s\")\n",
        "\n",
        "    # Load test images\n",
        "    print(\"Loading test images...\")\n",
        "    start_time = time.time()\n",
        "    X_test, y_test, valid_test_idx = load_image_batch(\n",
        "        file_paths, file_labels, test_idx, IMG_SIZE\n",
        "    )\n",
        "    print(f\"Loaded {len(X_test)} test images in {time.time() - start_time:.2f}s\")\n",
        "\n",
        "    if len(X_train) == 0 or len(X_test) == 0:\n",
        "        print(\"Error: No valid images loaded!\")\n",
        "        return None\n",
        "\n",
        "    # Convert labels to categorical (one-hot encoding) for CNN training\n",
        "    y_train_cat = to_categorical(y_train, num_classes=4)\n",
        "    y_test_cat = to_categorical(y_test, num_classes=4)\n",
        "\n",
        "    # Light data augmentation\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rotation_range=5,\n",
        "        width_shift_range=0.05,\n",
        "        height_shift_range=0.05,\n",
        "        horizontal_flip=True,\n",
        "        zoom_range=0.05,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    # Optimized callbacks\n",
        "    callbacks = [\n",
        "        EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=15,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1,\n",
        "            mode='max'\n",
        "        ),\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_accuracy',\n",
        "            factor=0.5,\n",
        "            patience=8,\n",
        "            min_lr=0.00001,\n",
        "            verbose=1,\n",
        "            mode='max'\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            'best_brain_tumor_model_swish.h5',\n",
        "            monitor='val_accuracy',\n",
        "            save_best_only=True,\n",
        "            verbose=1,\n",
        "            mode='max'\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Train the CNN model\n",
        "    print(\"Starting CNN training with Swish activation...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_datagen.fit(X_train)\n",
        "\n",
        "    history = cnn_model.fit(\n",
        "        train_datagen.flow(X_train, y_train_cat, batch_size=batch_size),\n",
        "        steps_per_epoch=len(X_train) // batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=(X_test, y_test_cat),\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    cnn_training_time = time.time() - start_time\n",
        "    print(f\"CNN training completed in {cnn_training_time:.2f}s\")\n",
        "\n",
        "    # Load the best CNN model weights\n",
        "    print(\"Loading best CNN model weights...\")\n",
        "    best_cnn_model = keras.models.load_model('best_brain_tumor_model_swish.h5')\n",
        "\n",
        "    # Create feature extractor from the best CNN model\n",
        "    print(\"Creating feature extractor from best CNN weights...\")\n",
        "    feature_extractor = create_feature_extractor(best_cnn_model)\n",
        "\n",
        "    # Extract features for XGBoost training using batch processing\n",
        "    print(\"Extracting features for XGBoost training...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Use batch-wise feature extraction to avoid memory issues\n",
        "    print(\"Extracting training features in batches...\")\n",
        "    X_train_features = extract_features_in_batches(feature_extractor, X_train, batch_size=32)\n",
        "\n",
        "    # Force garbage collection\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"Extracting test features in batches...\")\n",
        "    X_test_features = extract_features_in_batches(feature_extractor, X_test, batch_size=32)\n",
        "\n",
        "    # Force garbage collection\n",
        "    gc.collect()\n",
        "\n",
        "    feature_extraction_time = time.time() - start_time\n",
        "    print(f\"Feature extraction completed in {feature_extraction_time:.2f}s\")\n",
        "    print(f\"Feature shape: {X_train_features.shape}\")\n",
        "\n",
        "    # Standardize features for XGBoost\n",
        "    print(\"Standardizing features for XGBoost...\")\n",
        "    scaler = StandardScaler()\n",
        "    X_train_features_scaled = scaler.fit_transform(X_train_features)\n",
        "    X_test_features_scaled = scaler.transform(X_test_features)\n",
        "\n",
        "    # Train XGBoost with parameter tuning\n",
        "    print(\"Training XGBoost with parameter tuning...\")\n",
        "    best_xgb_model, best_xgb_results, best_params = train_xgboost_with_tuning(\n",
        "        X_train_features_scaled, y_train, X_test_features_scaled, y_test\n",
        "    )\n",
        "\n",
        "    # Get CNN accuracy from history\n",
        "    train_accuracies = history.history['accuracy']\n",
        "    val_accuracies = history.history['val_accuracy']\n",
        "    highest_train_acc = max(train_accuracies)\n",
        "    highest_val_acc = max(val_accuracies)\n",
        "\n",
        "    # Results dictionary\n",
        "    results = {\n",
        "        'cnn_highest_train_acc': highest_train_acc,\n",
        "        'cnn_highest_val_acc': highest_val_acc,\n",
        "        'xgb_accuracy': best_xgb_results['accuracy'],\n",
        "        'xgb_precision': best_xgb_results['precision'],\n",
        "        'xgb_recall': best_xgb_results['recall'],\n",
        "        'xgb_f1': best_xgb_results['f1'],\n",
        "        'xgb_mcc': best_xgb_results['mcc'],\n",
        "        'cnn_training_time': cnn_training_time,\n",
        "        'xgb_training_time': best_xgb_results['training_time'],\n",
        "        'feature_extraction_time': feature_extraction_time,\n",
        "        'xgb_prediction_time': best_xgb_results['prediction_time'],\n",
        "        'n_train_samples': len(X_train),\n",
        "        'n_test_samples': len(X_test),\n",
        "        'n_features': X_train_features.shape[1],\n",
        "        'y_true': y_test,\n",
        "        'y_pred': best_xgb_results['y_pred'],\n",
        "        'y_pred_proba': best_xgb_results['y_pred_proba'],\n",
        "        'cnn_model': best_cnn_model,\n",
        "        'feature_extractor': feature_extractor,\n",
        "        'xgb_model': best_xgb_model,\n",
        "        'best_xgb_params': best_params,\n",
        "        'scaler': scaler,\n",
        "        'cnn_history': history\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "# Display results without graphs\n",
        "def display_results(results):\n",
        "    \"\"\"\n",
        "    Display results from CNN + XGBoost hybrid model (text only)\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"CNN TRAINING RESULTS (WITH SWISH ACTIVATION)\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Highest CNN Training Accuracy: {results['cnn_highest_train_acc']:.4f}\")\n",
        "    print(f\"Highest CNN Validation Accuracy: {results['cnn_highest_val_acc']:.4f}\")\n",
        "    print(f\"CNN Training Time: {results['cnn_training_time']:.2f}s\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"CNN + XGBOOST HYBRID CLASSIFICATION RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"XGBoost Accuracy: {results['xgb_accuracy']:.4f}\")\n",
        "    print(f\"XGBoost Precision: {results['xgb_precision']:.4f}\")\n",
        "    print(f\"XGBoost Recall: {results['xgb_recall']:.4f}\")\n",
        "    print(f\"XGBoost F1-Score: {results['xgb_f1']:.4f}\")\n",
        "    print(f\"Matthews Correlation Coefficient: {results['xgb_mcc']:.4f}\")\n",
        "\n",
        "    print(f\"\\nDataset Information:\")\n",
        "    print(f\"Training samples: {results['n_train_samples']}\")\n",
        "    print(f\"Test samples: {results['n_test_samples']}\")\n",
        "    print(f\"Features extracted: {results['n_features']}\")\n",
        "\n",
        "    print(f\"\\nTiming Information:\")\n",
        "    print(f\"Feature extraction time: {results['feature_extraction_time']:.2f}s\")\n",
        "    print(f\"XGBoost training time: {results['xgb_training_time']:.2f}s\")\n",
        "    print(f\"XGBoost prediction time: {results['xgb_prediction_time']:.2f}s\")\n",
        "    print(f\"Total training time: {results['cnn_training_time'] + results['xgb_training_time']:.2f}s\")\n",
        "\n",
        "    print(f\"\\nBest XGBoost Parameters:\")\n",
        "    for param, value in results['best_xgb_params'].items():\n",
        "        print(f\"  {param}: {value}\")\n",
        "\n",
        "    # Classification report\n",
        "    print(f\"\\nDetailed Classification Report:\")\n",
        "    print(classification_report(results['y_true'], results['y_pred'], target_names=labels, digits=4))\n",
        "\n",
        "    # Confusion Matrix (text format)\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    cm = confusion_matrix(results['y_true'], results['y_pred'])\n",
        "    print(\"Predicted ->\")\n",
        "    print(f\"{'Actual':<12} {'Glioma':<8} {'Meningioma':<12} {'No Tumor':<10} {'Pituitary':<10}\")\n",
        "    print(\"-\" * 60)\n",
        "    for i, label in enumerate(labels):\n",
        "        print(f\"{label:<12} {cm[i][0]:<8} {cm[i][1]:<12} {cm[i][2]:<10} {cm[i][3]:<10}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FINAL SUMMARY - CNN + XGBOOST HYBRID MODEL\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Feature Extractor: Enhanced MobileNet CNN with Swish activation\")\n",
        "    print(f\"Classifier: XGBoost (optimized parameters)\")\n",
        "    print(f\"CNN Highest Training Accuracy: {results['cnn_highest_train_acc']:.4f}\")\n",
        "    print(f\"CNN Highest Validation Accuracy: {results['cnn_highest_val_acc']:.4f}\")\n",
        "    print(f\"XGBoost Final Test Accuracy: {results['xgb_accuracy']:.4f}\")\n",
        "    print(f\"XGBoost Final Precision: {results['xgb_precision']:.4f}\")\n",
        "    print(f\"XGBoost Final Recall: {results['xgb_recall']:.4f}\")\n",
        "    print(f\"XGBoost Final F1-Score: {results['xgb_f1']:.4f}\")\n",
        "    print(f\"Matthews Correlation Coefficient: {results['xgb_mcc']:.4f}\")\n",
        "    print(f\"Total Training Time: {results['cnn_training_time'] + results['xgb_training_time']:.2f}s\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Main function with CNN + XGBoost hybrid approach\n",
        "def main_hybrid(test_size=0.2, epochs=30):\n",
        "    \"\"\"\n",
        "    Main function to run CNN + XGBoost hybrid model for brain tumor classification\n",
        "    \"\"\"\n",
        "    print(\"Creating file paths list...\")\n",
        "    file_paths, file_labels = create_file_paths_list()\n",
        "\n",
        "    # Train CNN + XGBoost hybrid model\n",
        "    results = train_cnn_xgboost_hybrid(\n",
        "        file_paths, file_labels,\n",
        "        test_size=test_size,\n",
        "        batch_size=16,\n",
        "        epochs=epochs\n",
        "    )\n",
        "\n",
        "    if results is None:\n",
        "        print(\"Failed to train hybrid model!\")\n",
        "        return None\n",
        "\n",
        "    # Display results\n",
        "    display_results(results)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run the CNN + XGBoost hybrid model\n",
        "if __name__ == \"__main__\":\n",
        "    # Run CNN + XGBoost hybrid model with parameter tuning\n",
        "    results = main_hybrid(test_size=0.2, epochs=30)"
      ]
    }
  ]
}